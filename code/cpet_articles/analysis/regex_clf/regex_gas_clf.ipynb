{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0b9a414-5808-4449-a0be-04a63935840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from langdetect import detect\n",
    "# import sys\n",
    "# sys.path.append('code/cpet_articles/analysis/')\n",
    "# from article_screening_re import *\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "# from nltk.probability import FreqDist\n",
    "\n",
    "# import requests\n",
    "# import pickle\n",
    "# import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3923ecb-9c24-4e9c-b977-8bebb846f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = glob.glob('/Users/antonhesse/Desktop/Anton/Education/UMN/Lab and Research/HSPL/CPET_scoping_review/data/cpet_articles/full_texts/txts/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b104be9b-7f4d-44b2-b1f3-41ac5edb5d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file_one_string(file_name, file_list):\n",
    "    txt_re = re.compile(file_name)\n",
    "    fname = list(filter(txt_re.search, file_list))[0]\n",
    "    \n",
    "    # check file size to make sure the txt file actually has text\n",
    "    file_size = 0\n",
    "    while file_size == 0:\n",
    "        file_size = Path(fname).stat().st_size\n",
    "        if file_size != 0: # check if conversion to txt didn't work\n",
    "            with open(fname, 'r') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print('Empty file, returning None')\n",
    "            return None\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    return text_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcf0993e-8170-496e-825c-843a0cb8486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_file(file_name, file_list, mode = 'lemm'):\n",
    "    txt_re = re.compile(file_name)\n",
    "    fname = list(filter(txt_re.search, file_list))[0]\n",
    "    \n",
    "    # check file size to make sure the txt file actually has text\n",
    "    file_size = 0\n",
    "    while file_size == 0:\n",
    "        file_size = Path(fname).stat().st_size\n",
    "        if file_size != 0: # check if conversion to txt didn't work\n",
    "            with open(fname, 'r') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print('Empty file, returning None')\n",
    "            return None\n",
    "    text_lower = text.lower()\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    if mode == 'lemm':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "\n",
    "        return lemmatized_words\n",
    "    \n",
    "    elif mode == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_words = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    \n",
    "        return stemmed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2b5f4557-44be-4cd4-b647-9025efe82a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oxygen_uptake_re(text):\n",
    "    o2_uptake_consupmtion_re = re.compile(r'oxygen.{0,5}(uptake|consumption)', re.DOTALL)\n",
    "    vo2max_peak_re = re.compile(r'(v)?o2.{0,2}(max|peak)?', re.DOTALL)\n",
    "    aerobic_re = re.compile(r'(?<!an)aerobic.{0,2}(power|capacity)', re.DOTALL)\n",
    "    \n",
    "    mo_list = [\n",
    "        o2_uptake_consupmtion_re.search(text),\n",
    "        vo2max_peak_re.search(text),\n",
    "        aerobic_re.search(text)]\n",
    "    \n",
    "    mentions_o2_uptake = any(mo is not None for mo in mo_list)\n",
    "    \n",
    "    return mentions_o2_uptake\n",
    "\n",
    "def gas_collection_methods_re(text):\n",
    "    bbb_re = re.compile(r'breath.{0,5}breath', re.DOTALL)\n",
    "    douglas_bag_re = re.compile(r'douglas.{0,5}bag', re.DOTALL)\n",
    "    mixing_chamber_re = re.compile(r'mixing.{0,5}chamber', re.DOTALL)\n",
    "    \n",
    "    mo_list = [bbb_re.search(text), douglas_bag_re.search(text), mixing_chamber_re.search(text)]\n",
    "    \n",
    "    gas_methods = any(mo is not None for mo in mo_list)\n",
    "    \n",
    "    return gas_methods\n",
    "\n",
    "def vo2_units_re(text):\n",
    "    vo2_rel_re = re.compile(r'ml([^a-zA-Z]*kg[^a-zA-Z]*min|[^a-zA-Z]*min[^a-zA-Z]*kg)')\n",
    "    # mL_min_kg_re = re.compile(r'ml[^a-zA-Z]*min[^a-zA-Z]*kg')\n",
    "    \n",
    "    # L_mL_min = re.compile(r'(m)?l[^a-zA-Z]*min')\n",
    "\n",
    "    mo_list = [vo2_rel_re.search(text)]\n",
    "    \n",
    "    vo2_units = any(mo is not None for mo in mo_list)\n",
    "    \n",
    "    return vo2_units\n",
    "\n",
    "def estimated_vo2_re(text):\n",
    "    est_o2_uptake_re = re.compile(r'''(\n",
    "    (estimat|indirect|calculat).{0,30}oxygen.{0,2}(uptake|consumption)|\n",
    "    oxygen.{0,2}(uptake|consumption).{0,30}(estimat|indirect|calculat)\n",
    "    )''',\n",
    "                                           re.DOTALL | re.VERBOSE)\n",
    "    \n",
    "    est_vo2_re = re.compile(r'''(\n",
    "    (estimat|indirect|calculat).{0,30}(v)?o2.{0,2}(max|peak)|\n",
    "    (v)?o2.{0,2}(max|peak).{0,30}(estimat|indirect|calculat)\n",
    "    )''',\n",
    "                            re.DOTALL | re.VERBOSE)\n",
    "    \n",
    "    est_vo2_units_re = re.compile(r'''(\n",
    "    (estimat|indirect|calculat).{0,30}ml([^a-zA-Z]*kg[^a-zA-Z]*min|[^a-zA-Z]*min[^a-zA-Z]*kg)|\n",
    "    ml([^a-zA-Z]*kg[^a-zA-Z]*min|[^a-zA-Z]*min[^a-zA-Z]*kg).{0,30}(estimat|indirect|calculat)\n",
    "    )''',\n",
    "                            re.DOTALL | re.VERBOSE)\n",
    "    \n",
    "    mo_list = [est_o2_uptake_re.search(text), est_vo2_re.search(text), est_vo2_units_re.search(text)]\n",
    "    est_vo2 = any(mo is not None for mo in mo_list)\n",
    "    \n",
    "    return est_vo2\n",
    "    # assessment of aerobic capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "207148f2-ce69-4755-b73b-91206748be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = random.choices(txt_files, k=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99485e2d-e04c-483c-9a29-49efb6dca685",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files_short = [re.search(r'(?<=/full_texts/txts/).*', f).group() for f in test_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "63ec1e5f-4f79-4c52-8f6a-1251acb503f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c37447693d842c4a29d399952eec051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "long_word_lists = [process_file_one_string(f, txt_files) for f in tqdm(test_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0f65a9f-3cf5-4cf3-93ea-3f1c7f9f9663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed6763f1c40480a8280d4ead9a5ef7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "token_lists = []\n",
    "\n",
    "for word_list in tqdm(long_word_lists):\n",
    "    tokens = word_tokenize(word_list)\n",
    "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmatized_words = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    token_lists.append(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8a160c1c-81a3-483d-a7c1-9e61b88fe428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'txt_file': test_files_short,\n",
    "                   'article_text': long_word_lists,\n",
    "                    'tokens': token_lists})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ef25ec4-7948-4f95-9c2c-a12d45cb94ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_word_len'] = df.apply(lambda x: np.mean(list(map(len, x['tokens']))), axis=1)\n",
    "df = df[df['avg_word_len'] > 1.5].reset_index(drop=True) # removes articles with pdf to txt conversion issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd20eb5a-7233-4737-be79-65306812e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = df.apply(lambda x: detect(x['article_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2c3a01a4-084e-4952-961f-38c3daf0d02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['language'].isin(['en', 'cy'])].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "565e6e11-9248-46bf-9ae4-00bcbc267f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['o2_uptake'] = df.apply(lambda x: oxygen_uptake_re(x['article_text']), axis=1)\n",
    "df['vo2_units'] = df.apply(lambda x: vo2_units_re(x['article_text']), axis=1)\n",
    "df['gas_collection_methods'] = df.apply(lambda x: gas_collection_methods_re(x['article_text']), axis=1)\n",
    "df['estimated_vo2'] = df.apply(lambda x: estimated_vo2_re(x['article_text']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79907c65-1c75-4798-9312-5641d253cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['o2_uptake'] == False] # these articles are VERY likely to NOT include gas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cf8c6145-84ca-4134-90e6-8a9bd575098b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(365, 9)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['gas_collection_methods'] == True].shape  # these articles are VERY likely to INCLUDE gas data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
