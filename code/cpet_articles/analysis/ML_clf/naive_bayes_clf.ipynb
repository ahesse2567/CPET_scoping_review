{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44580c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RepeatedStratifiedKFold\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB,ComplementNB\n",
    "from statistics import mean\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f66e159-d396-4e41-9252-644200c066fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name, file_list):\n",
    "    txt_re = re.compile(file_name)\n",
    "    fname = list(filter(txt_re.search, file_list))[0]\n",
    "    \n",
    "    # check file size to make sure the txt file actually has text\n",
    "    file_size = 0\n",
    "    while file_size == 0:\n",
    "        file_size = Path(fname).stat().st_size\n",
    "        if file_size != 0: # check if conversion to txt didn't work\n",
    "            with open(fname, 'r') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print('Empty file, returning None')\n",
    "            return None\n",
    "    text_lower = text.lower()\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ace8d77f-b887-4c3e-874a-29f8bfcfd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_text(file_name, file_list):\n",
    "    txt_re = re.compile(file_name)\n",
    "    fname = list(filter(txt_re.search, file_list))[0]\n",
    "    \n",
    "    # check file size to make sure the txt file actually has text\n",
    "    file_size = 0\n",
    "    while file_size == 0:\n",
    "        file_size = Path(fname).stat().st_size\n",
    "        if file_size != 0: # check if conversion to txt didn't work\n",
    "            with open(fname, 'r') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print('Empty file, returning None')\n",
    "            return None\n",
    "    text_lower = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c71fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file_paths = list(Path('/Users/antonhesse/Desktop/Anton/Education/UMN/Lab and Research/HSPL/CPET_scoping_review/data/cpet_articles/full_texts/txts').glob('*.txt'))\n",
    "txt_files = [path.stem for path in txt_file_paths]\n",
    "txt_files_df = pd.DataFrame({'doi_suffix': txt_files})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd4e99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_text_df = pd.read_csv('/Users/antonhesse/Desktop/Anton/Education/UMN/Lab and Research/HSPL/CPET_scoping_review/data/cpet_articles/Manual text analysis - Data.csv')\n",
    "# manual_text_df['txt_file_name'] = manual_text_df.apply(lambda x: str(x['doi_suffix'] + '.txt'), axis=1) # not sure I need this\n",
    "manual_text_df = manual_text_df[(manual_text_df['Eligible'] == 'e') & (~manual_text_df['Gas data'].isna())].drop_duplicates().reset_index(drop=True) # remove non-original research, animal studies, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13821947-c0a2-44d6-8466-9926ba9af7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = pd.merge(txt_files_df, manual_text_df, how='inner', on='doi_suffix') # only analyze files for which we have the txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95e2a6d5-d67b-4cc1-b0b7-3ab03296b86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(320, 18)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e4e5e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:29<00:00, 10.87it/s]\n"
     ]
    }
   ],
   "source": [
    "words = [process_file(f, list(map(str, txt_file_paths))) for f in tqdm(merge_df['doi_suffix'].to_list())]\n",
    "joined_words = [' '.join(text) for text in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1cb754ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas_data = merge_df['Gas data'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d7361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ['The quick brown fox jumped over the lazy dog.',\n",
    "#        'The dog.',\n",
    "#        'The fox.']\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(joined_words, gas_data, test_size=0.25, random_state=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be1b8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbbff188",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test = vectorizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8844761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = vectorizer.fit_transform(joined_words)\n",
    "# vocabulary = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e7fb83c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BNB = BernoulliNB()\n",
    "BNB.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a5e64f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.6923076923076923\n",
      "confusion matrix:\n",
      "[[ 0  8]\n",
      " [ 0 18]]\n"
     ]
    }
   ],
   "source": [
    "BNB_pred = BNB.predict(features_test)\n",
    "accuracy_BNB = metrics.accuracy_score(labels_test, BNB_pred)\n",
    "print(\"BernoulliNB accuracy:\", accuracy_BNB)\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(labels_test, BNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f330fa89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "529af98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB accuracy: 0.6923076923076923\n",
      "confusion matrix:\n",
      "[[ 0  8]\n",
      " [ 0 18]]\n"
     ]
    }
   ],
   "source": [
    "MNB_pred = MNB.predict(features_test)\n",
    "accuracy_MNB = metrics.accuracy_score(labels_test, MNB_pred)\n",
    "print(\"BernoulliNB accuracy:\", accuracy_MNB)\n",
    "print(\"confusion matrix:\")\n",
    "print(metrics.confusion_matrix(labels_test, MNB_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5d0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector = TfidfVectorizer(stop_words='english')\n",
    "# vector.fit(joined_words)\n",
    "# vector.get_feature_names_out()\n",
    "# tfidf = vector.transform(joined_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cbe7bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_transformer = TfidfTransformer()\n",
    "# X_train_tfidf = tfidf_transformer.fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d06a17fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf = vector.transform(joined_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1eefbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', BernoulliNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bf4a5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_clf.fit(X = features_train, y = labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9899c628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<78x29823 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 112336 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
