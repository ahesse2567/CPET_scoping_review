{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d9e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, RepeatedStratifiedKFold\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB,ComplementNB\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa796f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_files = glob.glob('/Users/antonhesse/Desktop/Anton/Education/UMN/Lab and Research/HSPL/CPET_scoping_review/data/cpet_articles/txts/*.txt')\n",
    "# len(txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a960302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_size = 0\n",
    "# while file_size == 0:\n",
    "#     txt_file = random.choice(txt_files)\n",
    "#     file_size = Path(txt_file).stat().st_size\n",
    "#     if file_size != 0: # check if conversion to txt didn't work\n",
    "#         with open(txt_file, 'r') as f:\n",
    "#             text = f.read()\n",
    "#         print(txt_file)\n",
    "#     else:\n",
    "#         print('Skipping empty file')\n",
    "#         continue\n",
    "# text_lower = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98df532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = word_tokenize(text_lower)\n",
    "# tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3382a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "# filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "# filtered_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c947f138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps = PorterStemmer()\n",
    "# stemmed_words = [ps.stem(t) for t in filtered_tokens]\n",
    "# stemmed_words[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edfb7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer()\n",
    "# # with lemmatizing you get actual words. With stemming you don't often get actual words, but rather the root itself\n",
    "# lemmatized_words = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "# lemmatized_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f15e266",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_text_df = pd.read_csv('/Users/antonhesse/Desktop/Anton/Education/UMN/Lab and Research/HSPL/CPET_scoping_review/data/cpet_articles/Manual text analysis - Data.csv')\n",
    "manual_text_df['txt_file_name'] = manual_text_df.apply(lambda x: str(x['Article'] + '.txt'), axis=1)\n",
    "analyzed_txt_files = manual_text_df['txt_file_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc42fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_name, file_list):\n",
    "    txt_re = re.compile(file_name)\n",
    "    fname = list(filter(txt_re.search, file_list))[0]\n",
    "    \n",
    "    # check file size to make sure the txt file actually has text\n",
    "    file_size = 0\n",
    "    while file_size == 0:\n",
    "        file_size = Path(fname).stat().st_size\n",
    "        if file_size != 0: # check if conversion to txt didn't work\n",
    "            with open(fname, 'r') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            print('Empty file, returning None')\n",
    "            return None\n",
    "    text_lower = text.lower()\n",
    "    tokens = word_tokenize(text_lower)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n",
    "    \n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f7c133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84850d15ae14087b9f0d52264589673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "words = [process_file(f, txt_files) for f in tqdm(analyzed_txt_files)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc70693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45895590c3c54817a80b15722a47f28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "manual_text_df['words'] = words\n",
    "documents = []\n",
    "for idx, row in manual_text_df.iterrows():\n",
    "    documents.append((row['words'], row['Gas data']))\n",
    "random.seed(2312)\n",
    "random.shuffle(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e41fed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1f6390bd0c4c49b0b1dc52e9057334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_words = []\n",
    "for doc in tqdm(documents):\n",
    "    for w in doc[0]:\n",
    "        all_words.append(w)\n",
    "fdist = FreqDist(all_words)\n",
    "word_features = list(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2cb6622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features(document, ref_features):\n",
    "    words = set(document)\n",
    "    features = {}\n",
    "    for w in ref_features:\n",
    "        features[w] = (w in words)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c68f0307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd9a076a38d4623b0e8c72c2808b7b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_sets = [(find_features(words, word_features), gas) for (words, gas) in tqdm(documents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "80e0c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=None)\n",
    "# skf = StratifiedKFold(n_splits=5, random_state=None)\n",
    "# CNB = ComplementNB()\n",
    "# classifier = nltk.NaiveBayesClassifier()\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5953cbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc014aad34cb4bd3a104f0a97be92421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_folds = 5\n",
    "subset_size = int(len(feature_sets)/num_folds)\n",
    "CNB_classifier = SklearnClassifier(ComplementNB())\n",
    "accuracy = []\n",
    "\n",
    "for i in tqdm(range(num_folds)):\n",
    "    test_set = feature_sets[i*subset_size:][:subset_size]\n",
    "    train_set = feature_sets[:i*subset_size] + feature_sets[(i+1)*subset_size:]\n",
    "    classifier = CNB_classifier.train(train_set)\n",
    "    accuracy.append(round(nltk.classify.accuracy(classifier, test_set)*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bb150c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compliment NB classifier accuracy: [75.0, 75.0, 62.5, 50.0, 87.5]\n",
      "Mean Compliment NB accuracy: 70.0\n"
     ]
    }
   ],
   "source": [
    "print('Compliment NB classifier accuracy:', accuracy)\n",
    "print('Mean Compliment NB accuracy:', mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "29f137a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d7e31dbb444ccfafbf4ad220b5de2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_folds = 5\n",
    "subset_size = int(len(feature_sets)/num_folds)\n",
    "BNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "accuracy = []\n",
    "\n",
    "for i in tqdm(range(num_folds)):\n",
    "    test_set = feature_sets[i*subset_size:][:subset_size]\n",
    "    train_set = feature_sets[:i*subset_size] + feature_sets[(i+1)*subset_size:]\n",
    "    classifier = BNB_classifier.train(train_set)\n",
    "    accuracy.append(round(nltk.classify.accuracy(classifier, test_set)*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "13acb974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB classifier accuracy: [75.0, 75.0, 62.5, 50.0, 87.5]\n",
      "Mean Bernoulli NB accuracy: 70.0\n"
     ]
    }
   ],
   "source": [
    "print('Bernoulli NB classifier accuracy:', accuracy)\n",
    "print('Mean Bernoulli NB accuracy:', mean(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7954e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 articles in training set\n",
      "38 articles in testing set\n"
     ]
    }
   ],
   "source": [
    "train_set = feature_sets[0:len(feature_sets)*2//3]\n",
    "print(f'{len(train_set)} articles in training set')\n",
    "test_set = feature_sets[len(feature_sets)*2//3:]\n",
    "print(f'{len(test_set)} articles in testing set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8567c73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0093ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier accuracy percent: 73.7\n"
     ]
    }
   ],
   "source": [
    "print(\"Classifier accuracy percent:\",round((nltk.classify.accuracy(classifier, test_set))*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b69df2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     may = False             nan : y      =     21.9 : 1.0\n",
      "                    0,05 = True              nan : y      =     17.0 : 1.0\n",
      "                      4a = True              nan : y      =     17.0 : 1.0\n",
      "                    also = False             nan : y      =     17.0 : 1.0\n",
      "                corporal = True              nan : y      =     17.0 : 1.0\n",
      "              discussion = False             nan : y      =     17.0 : 1.0\n",
      "                 housing = True              nan : y      =     17.0 : 1.0\n",
      "                inferior = True              nan : y      =     17.0 : 1.0\n",
      "                 shapiro = True              nan : y      =     17.0 : 1.0\n",
      "                   table = False             nan : y      =     17.0 : 1.0\n",
      "                    toda = True              nan : y      =     17.0 : 1.0\n",
      "                    2.02 = True              nan : y      =     12.1 : 1.0\n",
      "                    24.8 = True              nan : y      =     12.1 : 1.0\n",
      "                      5a = True              nan : y      =     12.1 : 1.0\n",
      "                   burst = True              nan : y      =     12.1 : 1.0\n",
      "                ciencias = True              nan : y      =     12.1 : 1.0\n",
      "                coaching = True              nan : y      =     12.1 : 1.0\n",
      "                 culture = True              nan : y      =     12.1 : 1.0\n",
      "                  custom = True              nan : y      =     12.1 : 1.0\n",
      "                 déficit = True              nan : y      =     12.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635afa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
