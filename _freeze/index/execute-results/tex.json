{
  "hash": "48fe6e3c1ae82221caa7e1094ba47412",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"The prevalence of gas exchange data processing methods: a semi-automated scoping review\"\n\n# Hiding author and affiliation info per IJSM instructions.\n\n# author:\n#   - name: Anton Hesse\n#     orcid: 0000-0001-8456-7343\n#     corresponding: true\n#     email: hesse151@umn.edu, ahesse2567@gmail.com\n#     roles:\n#       - Investigation\n#       - Software\n#       - Visualization\n#       - Writing – Original Draft Preparation\n#     affiliations:\n#       - University of Minnesota-Twin Cities\n#   - name: Manix White\n#     roles:\n#       - Data Collection\n#     affiliations:\n#       - University of Minnesota-Twin Cities\n#   - name: Christopher Lundstrom\n#     orcid: 0000-0002-1527-1685\n#     corresponding: false\n#     roles: \n#       - Supervision\n#       - Writing – Review & Editing\n#     affiliations:\n#       - University of Minnesota-Twin Cities\nabstract: |\n  Cardiopulmonary exercise testing involves collecting variable breath-by-breath data, sometimes requiring data processing of outlier removal, interpolation, and averaging before later analysis. These data processing choices, such as averaging duration, affect calculated values such as VO~2~max. However, assessing the implications of data processing without knowing popular methods worth comparing is difficult. In addition, such details aid study reproduction. We conducted a semi-automated scoping review of articles with exercise testing that collected data breath-by-breath from three databases. Of the 8,344 articles, 376 (mean 4.5%, 95% CI: 4.1-5.0%) and 581 (7.0%, 6.4-7.5%) described outlier removal and interpolation, respectively. A random subset of 1,078 articles revealed 60.9% (57.9-63.7%) reported averaging methods. Commonly documented outlier cutoffs were ± 3 or 4 SD (39.1% and 51.6%, respectively). The dominating interpolation duration and procedure were one second (93.9%) and linear interpolation (92.5%). Averaging methods commonly described were 30 (30.9%), 60 (12.4%), 15 (11.6%), 10 (11.0%), and 20 (8.1%) second bin averages. This shows that studies collecting breath-by-breath data often lack detailed descriptions of data processing methods, particularly for outlier removal and interpolation. While averaging methods are more commonly reported, improved documentation across all processing steps will enhance reproducibility and facilitate future research comparing data processing choices.\nkeywords:\n  - Data Averaging\n  - Outlier Removal\n  - Interpolation\n# author-note:\n#   disclosures:\n#     conflict of interest: \"The author has no conflict of interest to declare.\"\n# date: last-modified\nbibliography: references.bib\n# citation:\n#   container-title: \"JOURNAL GOES HERE\"\nnumber-sections: true\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   3.5.1     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.1\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(knitr)\nlibrary(flextable)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'flextable'\n\nThe following object is masked from 'package:purrr':\n\n    compose\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# library(kableExtra)\nextrafont::loadfonts(quiet = TRUE)\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nhere() starts at /Users/antonhesse/Desktop/Anton/Education/UMN/PhD/Dissertation/CPET_scoping_review\n```\n\n\n:::\n:::\n\n\n\n\n\n# Introduction\n\nClinicians and researchers use cardiopulmonary exercise testing (CPET) to determine maximal aerobic capacity (VO~2~max), ventilatory thresholds, and VO~2~ kinetics.\nSuch values help categorize fitness, predict disease risk, and guide exercise [@pescatello2014].\nUsing CPET results to guide exercise, especially relative to thresholds, yields better improvements given more consistent and predictable metabolic responses [@jamnick2020].\nTherefore, incorrectly calculating or identifying these values limits CPET benefits.\n\nBreath-by-breath (BBB) data in CPET often requires processing to manage its high variability [@robergs2010].\nBBB VO~2~ data can change by up to 86% during a steady state, but changes to muscle blood flow or oxygen extraction cannot account for such rapid fluctuations [@robergs2010].\nInstead, Robergs et at.\n[@robergs2010] showed that the rate and depth of breathing account for most of the variation in VO~2~ during both steady states and incremental exercise.\nTherefore, CPET data processing usually involves outlier removal, optional interpolation to regular intervals, and averaging to more accurately reflect whole-body metabolism [@robergs2010].\nPrevious research has shown that data averaging influences CPET values.\nAveraging over longer durations reduces VO~2~max and VO~2~ plateau detection [@robergs2003; @robergs2010; @sousa2010; @johnson1998; @sell2021; @midgley2007; @astorino2009; @astorino2000; @martin-rincon2019; @martin-rincon2020; @scheadler2017; @dejesus2014; @hill2003; @smart2015; @matthews1987].\nThe importance of attaining a VO~2~ plateau to accurately determine VO~2~max has been highlighted because \"secondary\" VO~2~max criteria often underestimate VO~2~max [@poole2017], and thus misclassify cardiorespiratory fitness.\nFinally, we are unaware of research on the effects of data processing and locating ventilatory thresholds.\n\nOutlier removal typically excludes points beyond 2-4 standard deviations (SD) from the local mean [@nolte2023].\nThese cutoffs are common because the relatively small sample size of BBB gas exchange data often contains more values beyond 3 or 4 SD than one would predict from an assumed Gaussian distribution [@lamarra1987].\nMore outliers appear than expected because of both conscious and unconscious alterations of breathing patterns, including swallowing and coughing [@lamarra1987].\nWe are unaware of prior research that examines how different outlier removal strategies affect VO~2~max, ventilatory thresholds, and VO~2~ kinetics.\n\nInterpolation, often to one-second intervals, is common in VO~2~ kinetics research to \"ensemble\" average repeated transitions to minimize variability [@keir2014; @lamarra1987].\nAlthough this does not affect parameter estimates, one-second interpolation has been criticized for artificially narrowing confidence intervals as respiratory rates are usually below 60 breaths per minute, even near maximal exercise [@benson2017; @francescato2014; @francescato2019; @francescato2015].\nAs before, we are unaware of research specifically investigating how interpolation affects VO~2~max and ventilatory threshold identification.\n\nData processing choices, such as averaging and interpolation, impact CPET variables or their confidence intervals.\nEarlier surveys [@robergs2010] and studies [@midgley2007] were small and focused on averaging methods only, finding time-based bin averages (e.g., 30-second averages) were popular.\nA more recent scoping review by Nolte et al. @nolte2023 found that nearly half of studies on VO2max ramp protocols lacked data processing steps.\nThey also found that only 4.3% and 4.5% of papers reported interpolation and outlier removal strategies, respectively.\nFinally, they reported that scant studies employed the recommended moving-average or digital filter averaging options suggested in 2010 [@robergs2010].\n\nThese low rates of reporting data processing steps may hamper reproduction or replication attempts, which have become a more prominent issue in science within the past decade [@goodman2016; @opensciencecollaboration2015].\nIn addition, as summarized by Nolte et al. [-@nolte2023], using VO~2~max and similar values to classify fitness or evaluating patients for treatment requires practitioners to consider and state data processing choices as they may inadvertently misclassify or suboptimally select patients and treatments.\n\nThis research expands on that of Nolte et al. @nolte2023 but searches without date restriction and includes studies with CPET data beyond VO~2~max ramp tests.\nTo accomplish greater breadth, we employed a semi-automated analysis to find more articles based on common text patterns before manually reading extracted subsections from each study.\nThis review assesses the reporting frequency of outlier removal, interpolation, and averaging methods.\nThe results emphasize the need for documentation to improve reproducibility and documents the data processing choices worth testing in future research investigating the effects of such choices on CPET values.\n\n# Methods {#sec-methods}\n\n## Protocol and Registration\n\nThis report followed PRISMA scoping review and related guidelines [@tricco2018; @peters2020; @page2021]. This work was first registered with the Open Science Framework [@foster2017; -@hesse2022] and is based on a dissertation chapter by the first author [-@hesse2023].\nThe code and most data for this project are available on [GitHub](https://github.com/ahesse2567/CPET_scoping_review/).\n\n## Eligibility Criteria\n\nThis scoping review surveyed gas exchange data processing choices in original, peer-reviewed studies, summarizing the reporting frequency and methods for outlier removal, interpolation, and averaging.\nFull-text files could not be shared due to licensing restrictions.\nEligible articles were original, peer-reviewed articles, with BBB gas exchange data, human participants, in English, with a DOI.\nWe imposed no date restriction to be as comprehensive as possible.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsource(here(\"code/cpet_articles/analysis/reporting/PRISMA_flowchart_calcs.R\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nJoining with `by = join_by(doi_suffix)`\nJoining with `by = join_by(doi_suffix)`\nJoining with `by = join_by(doi_suffix)`\nJoining with `by = join_by(doi_suffix)`\nJoining with `by = join_by(doi_suffix)`\n```\n\n\n:::\n:::\n\n\n\n\n\n## Information Sources and Search\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmanual_downloads <- read_csv(\n    here::here(\"data/cpet_articles/Manual Downloads - Articles.csv\"),\n    show_col_types = FALSE)\n\nn_unsubscribed <- manual_downloads %>% \n    count(subscribed) %>% \n    filter(subscribed == FALSE) %>% \n    select(n) %>% \n    pull()\n```\n:::\n\n\n\n\n\nData were collected from Ovid-MEDLINE, Scopus, and Web of Science on 2022-06-27 with librarian assistance.\nThe electronic search strategies for all databases can be found in the information sources and search section of the supplemental materials.\n\nOur search output comprised article identifiers like DOIs.\nTo find missing DOIs, we employed the PubMed Central ID Converter API [@ncbi2022] using Python.\nFull texts were accessed via publisher text and data mining APIs using Python, unpaywall.org [@unpaywal] using the unpywall Python package, through custom-built web-scraping scripts, or manually.\nOur library subscription did not permit access to 1,549 articles.\n\n## Selection of Sources of Evidence\n\nThis study used a single screening process, requiring only BBB gas exchange data collection with exercise.\nThe corresponding PRISMA flow diagram was created using the PRISMA2020 R package [@haddawayPRISMA2020PackageShiny2022].\n\n### Text Analysis & Screening\n\nDespite database search filters, we screened additional non-English, non-human, and non-original articles such as reviews, meta-analyses, and protocol registrations, in addition to case studies.\nWe manually analyzed a subset of articles to help build machine learning (ML) classifiers and construct regular expressions (RegExs) described below.\nRegular expressions identify specific text sequences.\nA familiar RegEx example is searching a document using cmd/ctrl+F.\nThese ML classifiers and RegExs helped identify ineligible articles.\nThis computerized screening required converting full-text PDF and EPUB documents into plain text files.\nPlain text files were normalized by transforming text to lowercase, removing hyphenations and extra whitespace, and correcting some plain text conversion-induced errors.\n\nFollowing the normalization, we identified and removed articles that failed to correctly convert into text format, spotted non-English articles using the fasttext Python module [@bojanowski2016] and employed a random forest classifier from the sklearn Python package [@pedregosa2011] to detect ineligible articles based on our criteria.\nSee the supplemental methods for text analysis and screening for additional ML model details.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nineligible_articles <- read_csv(\n    \"data/cpet_articles/text_analysis/eligibility/ineligible_articles_combined.csv\",\n    show_col_types = FALSE)\n\nbbb_articles <- read_csv(\n    \"data/cpet_articles/text_analysis/bbb_articles.csv\",\n    show_col_types = FALSE) %>% \n    distinct(doi_suffix, .keep_all = TRUE) %>% \n    filter(!(doi_suffix %in% ineligible_articles$doi_suffix))\n\nn_bbb_articles <- nrow(bbb_articles)\n```\n:::\n\n\n\n\n\nNext, we identified BBB articles using RegExs.\nArticles were considered BBB articles if their text contained variations of the phrase \"breath-by-breath\", or if their text included the make or model of a known BBB analyzer.\nBreath-by-breath brands and analyzers we included were Oxycon and Carefusion brands, Medgraphics Ultima, CPX, CCM, and CardiO~2~ models, Sensormedics Encore and 2900 models, Cosmed quark, k4, and k5 models, and the Minato RM-200, AE-280S, AE-300S, and AE-310S models.\nSome metabolic carts have both BBB and mixing chamber modes.\nIf not described we assumed the data was collected BBB.\nIn total, we identified 8,412 articles.\n\nWithin this subset, we performed a similar RegEx search for studies that documented using Douglas Bags or mixing chambers and excluded those articles.\nThe full details are described in the \"data charting process\" section.\n\n### Data Charting Process\n\nRegExs identified short phrases likely indicating that the authors described these methodological details.\nIf present, we extracted a \"snippet\" of text surrounding those phrases for later manual analysis by obtaining approximately 200 surrounding characters.\nWe then recorded the methods from these snippets.\nIn all cases, methods were only considered documented if the snippets provided at least some specific information.\nFor example, articles stating outlying breaths were removed but without describing the outlier criteria were considered \"not described.\" Finally, we read the full-text article to accurately document the data when snippets were ambiguous.\nFull-text articles without snippets were not read and there methods were documented as \"not described.\"\n\nThe data charting subsections below provide text extraction examples.\nExtracted texts were normalized to lowercase, with end-of-line hyphenation and unnecessary white space removed before capitalizing certain keywords for readability.\nTherefore, formatting varies and may include unconventional spacing and Unicode characters.\nFinally, the snippets may not start or and at the beginning or end of a word or sentence because the RegExs extracted a specific number of characters rather than words.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nz <- qnorm(0.025, lower.tail = FALSE)\nmargin_of_error <- 0.03\np <- 0.5\n\nn_articles <- ceiling(((margin_of_error / z)^2)^-1 * (p * (1 - p)))\n\nn_rounded_up_nearest_hundred <- round(n_articles, -2)\n```\n:::\n\n\n\n\n\nAll eligible BBB articles were analyzed for outlier and interpolation methods due to distinct descriptions and fewer total articles (\\~5%).\nIn contrast, we analyzed a random subset of articles using a random number generator in Python to document data averaging methods because far more articles described their averaging methods.\nEarly estimates as we developed our RegExs were that \\~60% or 5,047 articles had some averaging details.\nFurthermore, the phrases associated with averaging methods are more generic and often refer to other study aspects, such as heart rate averaging periods.\nGiven the large number of articles, we needed a minimum sample size of 1,068 based on a 95% confidence interval and a maximum margin of error (MOE) of ±3%, assuming a proportion of 0.5 for a conservative estimate.\nHowever, we raised this to 1,100 in anticipation of finding ineligible articles that eluded our previous text screening.\nThe chosen MOE was selected as a balance between accuracy and the required corresponding samples: decreasing the MOE to ±2% with an assumed proportion of 0.5 would require another 1,333 samples.\n\n#### Outliers\n\nOur outlier RegExs identified phrases like \"swallowing\", \"coughing\", \"errant\", \"aberrant\", and references to the \"local mean,\" \"prediction interval,\" or a specific standard deviation limit such as ±3 or ±4.\nFor example, our RegExs found \" errant\"; \" local mean\"; and \"breath-by-breath ̇vo2 data from each step transition were initially edited to exclude errant breaths by removing values lying more than 4 sd\" from @breese2019.\nWe gathered snippets surrounding those phrases and combined them when overlapping, thus producing\n\n> > y\\[hb+mb\\] data (quaresima & ferrari, 2009).\n> > expressed as 2.5 data analysis and kinetic modelling the breath-by-breath ̇vo2 data from each step transition were initially edited to exclude errant breaths by removing values lying more than 4 sd from the local mean determined using a five-breath rolling\\\\n\\\\x0c1932 breese et al. and deoxy\\[hb+mb\\] responses were subavera\n\nWe recorded the outlier limit as ±4 SD and the outlier function as a rolling 5-breath whole mean average.\n\n#### Interpolation\n\nNearly all articles describing interpolation methods used variations of \"interpolate.\" The remaining phrases were infrequent and inconsistent enough that interpolation methods were only described for those articles when discovered by chance.\nTo illustrate interpolation documentation, our RegExs extracted the snippet from @hartman2018.\n\n> > the v̇ o2 data from gd and gl exercise bouts were modeled to characterize the oxygen uptake kinetics following the methods described by bell et al. (2001).\n> > breath-by-breath v̇ o2 data were linearly INTERPOLATed to provide second-by-second values.\n> > phase 1 data (i.e. the cardiodynamic component), from the first ∼20 s of exercise, were omitted from the kinetics analysis because phase 1 is not directly repres\n\nWe documented the interpolation type as \"linear\" and the interpolation time as one second.\n\n#### Averaging\n\nWe document averaging methods according to five criteria: type/units, subtype/calculation, amount, measure of center, and mean type.\nType/units refer to the averaging units of time, breath, and digital filters.\nSubtype/calculation involves specific computations like bin and rolling averages or digital filter forms.\nThe amount is the unit quantity.\nFor example, 30 for a time average is 30 seconds but is 30 breaths for a breath average.\nMeasure of center distinguishes between mean or median, and mean type delineates whole vs. trimmed mean.\nTrimmed (truncated) means exclude a number of the highest and lowest values in the quantity before averaging the remaining data.\n\nDescriptions of averaging methods are also considerably more diverse and generic than outlier and interpolation descriptions.\nFor example, \"30-second averages\" and \"averaged every 30 seconds\" invite complexity, leading to more snippets referring to averaging something besides BBB gas exchange data.\nGiven that, we required that the text snippets include a reference to gas data such as the text \"O~2~,\" \"breath,\" \"gas,\" \"ventilation,\" etc.\n\nIn contrast to previous studies, we also documented every averaging method we found per paper instead of only describing the averaging method for VO~2~max.\nWe also recorded multiple averaging methods when the authors described the sampling interval and the transformation applied to it.\nFor example, the snippet from @hassinen2008\n\n> > ath method using the vmax respiratory gas analyzer (sensormedics, yorba linda, ca).\n> > vo2max was deﬁned as the mean of the three highest values of the averaged oxygen consumption measured consecutively OVER 20-S intervals.\n> > a total of 98% of the subjects achieved the respiratory exchange ratio of ⱖ1.1.\n> > electrocardiography was recorded throughout the exercise test using cardiosoft software (ge medical systems,\n\nstates that oxygen consumption was measured every 20 seconds and that VO~2~max was calculated as the average of three 20-second intervals, or 60-seconds.\nFor this article, we documented one averaging method as a 20-second time bin whole mean and another as a 60-second time bin whole mean.\n\nIn many cases, authors did not explicitly use the terms \"average\" or \"mean\" to describe their averaging methods, but we documented their methods when implied.\nFor example, the snippet from @deboeck2004 reading\n\n> > red using a continuously monitored electrocardiograph.\n> > blood pressure was measured at the end of each workload increment using an automatic sphygmomanometer.\n> > peak v9o2 was deﬁned as the v9o2 measured DURING THE LAST 30 S of peak exercise.\n> > oxygen pulse was calculated by dividing v9o2 by cardiac frequency.\n> > the anaerobic threshold was detected using the v-slope method \\[16\\].\n> > the ventilatory equivalent for carbon dioxide w\n\nstates they calculated VO~2~peak using the last 30 seconds of exercise data.\nWe documented such phrasing as a 30-second time-bin whole mean average.\n\n### Data Items\n\nIn all cases, articles that did not return any phrases were documented as \"not described\" for their respective data processing category.\nIf snippets did not refer to the data processing category or if the snippet lacked sufficient information, those data processing variables were documented as \"not described.\" For example, interpolation variables were denoted as \"not described\" if interpolation was acknowledged but without details for the interpolation type or time.\n\n#### Outliers\n\nWe documented the outlier limit, for example, ±3 standard deviations, and any outlier function used to compute the outlier limit, if described.\n\n#### Interpolation\n\nWe recorded the interpolation type (linear, cubic, Lagrange, specifically *un*interpolated, and other) and time frame (e.g., every one second).\n\n#### Averaging\n\nWe noted the following averaging types: Time, breath, breath-time, time-breath, time-time, digital filter, ensemble, (explicitly) *un*averaged, and other.\nAveraging subtypes included bin, rolling, bin-roll, rolling-bin, Butterworth low-pass, Fast Fourier Transform (FFT), and Savitsky-Golay.\nNext, we recorded the time in seconds or the number of breaths.\nWe recorded the measure of center as mean or median.\nFinally, we noted if the mean was a whole or trimmed.\n\n### Synthesis of Results\n\nCounts, proportions, and Agresti-Coull 95% confidence intervals were calculated for the reporting frequency of each data processing method using R R version 4.1.2 [@rcoreteam2021] in the RStudio IDE version 2023.6.1.524 [@rcoreteam2021].\nWhen articles reported multiple methods, we only counted this article once for calculating overall reporting proportions.\n\n# Results {#sec-results}\n\n## Selection of Sources of Evidence\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsource(here::here(\"code/cpet_articles/analysis/reporting/avg_methods_reporting.R\"))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'scales'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:purrr':\n\n    discard\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n`summarise()` has grouped output by 'avg_type', 'avg_subtype', 'avg_amount',\n'avg_mos'. You can override using the `.groups` argument.\nSaving 5.5 x 3.5 in image\n```\n\n\n:::\n:::\n\n\n\n\n\nFigure 1 shows the selection of sources of evidence flowchart.\nThe initial search identified 50,730 articles; 21,715 remained for retrieval after removing duplicates and those without DOIs.\nA total of 8,344 articles analyzed were included in the interpolation and outlier analysis.\nAfter removing 22 ineligible articles that we discovered during data documentation from the original 1,100 random articles, we analyzed 1,078 articles for our averaging analysis.\n\n## Characteristics and Results of Individual Sources of Evidence\n\nGiven the vast nature of this scoping review, readers can view web links to our [outlier](https://docs.google.com/spreadsheets/d/1k_i4EP5U3zMltk8n21X-KHGoxUfR6XAJu6lxrVUufg0/edit?usp=sharing), [interpolation](https://docs.google.com/spreadsheets/d/1mNHwyNwVeQeAAm-Jx43ImR91sLyRLSvad9oglHQB83A/edit?usp=sharing), and [averaging](https://docs.google.com/spreadsheets/d/1KdmDZuI1FS1XUK5zJm3JIqf4tQiBd0C1pW0p0PFZweU/edit?usp=sharing) data charting spreadsheets.\n\n## Synthesis of Results\n\nWe present our results according to the reporting prevalence followed by the specific characteristics when reported.\n\n### Outliers\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsource(\"code/cpet_articles/analysis/reporting/outlier_reporting.R\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat <- vroom(...)\n  problems(dat)\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nSaving 5.5 x 3.5 in image\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nprop_3sd <- specified_outlier_cutoffs_by_type %>% \n    filter(outlier_limit == \"±3 SD / 99%\") %>% \n    select(prop) %>% \n    pull()\n\nprop_4sd <- specified_outlier_cutoffs_by_type %>% \n    filter(outlier_limit == \"±4 SD\") %>% \n    select(prop) %>% \n    pull()\n```\n:::\n\n\n\n\n\nOf the 8,344 articles, 376 (4.5%, 95% CI: 4.1-5.0%) reported outlier removal methods.\nThe most prevalent reported methods were ±3 (39.1%) and ±4 (51.6%) SD (Figure 2).\n\n\n\n\n\n::: {#cell-fig-specified-outlier-limits .cell}\n\n```{.r .cell-code .hidden}\n# plot(prop_outlier_limits_plot)\n```\n:::\n\n\n\n\n\nOnly 102 (1.2%, 95% CI: 1.0-1.5%) articles reported details of the function they used to calculate their outlier limit.\nOf those, breath-based averages (n = 76, 74.5%) then time-based averages (n = 15, 14.7%) were the most common for calculating outlier boundaries.\nSpecifically, 5-breath averages (n = 54, 52.9%) were the most prevalent functions to calculate outlier limits.\n\n### Interpolation\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsource(\"code/cpet_articles/analysis/reporting/interpolation_reporting.R\")\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\ni Please use `reframe()` instead.\ni When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nSaving 5.5 x 3.5 in image\n```\n\n\n:::\n:::\n\n\n\n\n\nOf 8,344 articles 581 (7.0%, 95% CI: 6.4-7.5 specified interpolation, with one-second intervals as the most common (93.9%).\nAround half of reported interpolation procedures included the method (n = 314, 54.0%), with linear interpolation as the most popular (n = 247, 92.5%) (Figure 3).\n\n\n\n\n\n::: {#cell-fig-interpolation_by-time-and_type .cell}\n\n```{.r .cell-code .hidden}\n# plot(condensed_interpolation_by_specified_procedure_plot)\n```\n:::\n\n\n\n\n\n### Averaging\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmost_popular_avg_type <- avg_by_type_tab %>% \n    filter(n == max(n)) %>% \n    select(avg_type) %>% \n    pull()\n\nmost_popular_avg_type_prop <- avg_by_type_tab %>% \n    filter(n == max(n)) %>% \n    select(prop) %>% \n    pull()\n\nsecond_most_popular_avg_type <- avg_by_type_tab %>% \n    filter(n == sort(n)[length(n) - 1]) %>%\n    select(avg_type) %>% \n    pull()\n\nsecond_most_popular_avg_type_prop <- avg_by_type_tab %>% \n    filter(n == sort(n)[length(n) - 1]) %>% \n    select(prop) %>% \n    pull()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmost_popular_avg_subtype <- avg_by_subtype_tab %>% \n    filter(n == max(n)) %>% \n    select(avg_subtype) %>% \n    pull()\n\nmost_popular_avg_subtype_prop <- avg_by_subtype_tab %>% \n    filter(n == max(n)) %>% \n    select(prop) %>% \n    pull()\n\nsecond_most_popular_avg_subtype <- avg_by_subtype_tab %>% \n    filter(n == sort(n)[length(n) - 1]) %>%\n    select(avg_subtype) %>% \n    pull()\n\nsecond_most_popular_avg_subtype_prop <- avg_by_subtype_tab %>% \n    filter(n == sort(n)[length(n) - 1]) %>% \n    select(prop) %>% \n    pull()\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\ntop_three_type_subtype <- avg_by_type_subtype_tab %>% \n    slice_max(order_by = n, n = 3) %>% \n    select(avg_type_subtype) %>% \n    pull() %>% \n    str_to_lower()\n\ntop_three_type_subtype_props <- avg_by_type_subtype_tab %>% \n    slice_max(order_by = n, n = 3) %>% \n    select(prop) %>% \n    pull()\n```\n:::\n\n\n\n\n\nWe recorded 656 (60.9%, 95% CI: 57.9-63.7%) articles with averaging methods.\nOf these, 14 articles reported more than one averaging method.\nTime averages dominated in popularity (91.5%) (Table 1).\nBin averages proved the most widespread averaging subtype (89.9%) (Table 1).\nTogether, time-bin (86.8%) was the most frequent type-subtype averaging method combination.\n\n\n\n\n\n::: {#tbl-avg_type_subtype_tables .cell .tbl-cap-location-top layout-ncol=\"2\" tbl-cap='Averaging methods by type (a) and subtype (b).'}\n\n```{.r .cell-code .hidden}\n# Subtable 1: Averaging Type\navg_by_type_tab_formatted <- avg_by_type_tab %>% \n    mutate(prop = round(prop * 100, 1)) %>% \n    rename(\"Averaging Type\" = avg_type,\n           N = n,\n           \"%\" = prop) %>% \n    flextable() %>% \n    flextable::add_header_row(values = \"Table 1a: Averaging type by counts (N) and proportions (%).\", colwidths = 3)\n\n# Subtable 2: Averaging Subtype\navg_by_subtype_tab_formatted <- avg_by_subtype_tab %>% \n    mutate(prop = round(prop * 100, 1),\n           avg_subtype = if_else(avg_subtype == \"Fft\",\n                                 \"Fast Fourier Transform\",\n                                 avg_subtype)) %>% \n    rename(\"Averaging Subtype\" = avg_subtype,\n           N = n,\n           \"%\" = prop) %>% \n    arrange(desc(N)) %>% \n    flextable() %>% \n    flextable::add_header_row(values = \"Table 1b: Averaging subtype by counts (N) and proportions (%).\", colwidths = 3)\n\nflextable::save_as_docx(\n  avg_by_type_tab_formatted,\n  avg_by_subtype_tab_formatted,\n  path = here::here(\"graphics/avg_tables.docx\"))\n```\n:::\n\n\n\n\n\nWhen incorporating averaging amounts, 30-, 60-, 15-, and 10-second bin averages (Figure 4) were the most popular.\nThe \"other\" methods category accounted for the second highest share of the total, but this represents many rarely used averaging methods.\n\n\n\n\n\n::: {#cell-fig-avg_full_method_plot .cell}\n\n```{.r .cell-code .hidden}\n# plot(avg_by_full_method_plot)\n```\n:::\n\n\n\n\n\n# Discussion {#sec-discussion}\n\n## Summary of Evidence\n\nThis review shows that gas exchange data processing methods are infrequently reported for outlier removal and interpolation.\nWe consider outlier removal documentation important as it applies to many exercise test analyses.\nOutlier removal is key for VO~2~ kinetics research requiring high temporal resolution.\nOutlier removal is also relevant for maximal exercise testing as outliers near the end of a test may influence VO~2~max or VO~2~peak.\nPrevious research indicates that a VO~2~max below the 20th percentile for age and sex increases the risk of all-cause mortality [@blair1995], so accurate determinations of VO~2~max are important for individuals with low cardiorespiratory fitness: an erroneous breath yielding an overestimated VO~2~max may subdue the urgency to improve cardiovascular health for low-fitness individuals.\nAlthough we are unaware of studies examining VO~2~max misclassification due to different outlier removal strategies, averaging duration can influence which patients are deemed eligible for heart transplantation [@johnson1998].\n\nOutliers could also affect mathematical VO~2~ plateau determinations.\nSuch methods test if neighboring VO~2~ values or a VO~2~ vs. time slope does not change or increase by more than a set rate (e.g., 50 mL/min) at the end of a maximal test.\n[@robergs2001; @astorino2000; @myers1989; @myers1990; @yoon2007].\nThough data averaging dampens their influence, outliers present near the conclusion of a maximal test could plausibly interfere with mathematical VO~2~ plateau determination.\n\nWe are currently unaware of research that has tested this, but outliers may interfere with submaximal thresholds found using algorithms, especially if they exist near likely breakpoints.\nThreshold algorithms often fit piecewise linear regressions and solve for the lowest sums of squares [@jones1984; @beaver1986; @orr1982].\nPoints near the edges of the regression lines have more leverage when solving for the best-fit line and, therefore, are more likely to influence the slope or intercept.\nSuch changes could alter the intersection point of the piecewise regression, and thus, the threshold values.\n\nFinally, even fewer articles reported the outlier limit calculation function.\nAs the function chosen impacts calculated outlier limit, it also affects where values are considered outliers.\nThe most popular strategy we observed were rolling breath averages and standard deviation with 3-5 breaths and ±3-4 SD.\nThough rarely noted, we suspect these are popular as they are based on published literature [@lamarra1987], are relatively easy to program, and may come pre-installed on some metabolic cart software.\nHowever, this should not preclude investigating other outlier functions within this domain or using digital filtering methods that specifically dampen high-frequency oscillations.\n\nLower interpolation reporting is expected, given its relevance primarily to less frequent VO~2~ kinetics studies.\nHowever, the V-slope method, one of the most common methods for determining the first ventilatory threshold, interpolates data in their original method [@beaver1986].\nImportantly, the V-slope algorithm is only part of the overall V-slope method, so it can be unclear if authors interpolated data when citing the V-slope method.\nGiven this and the artificial confidence interval shrinkage, we recommend authors specify interpolation details.\nAlthough the uncertainty of the VO~2~ at thresholds are not commonly reported, doing so with appropriate confidence intervals may be prudent as traditional thresholds may be better described as transitions [@pethick2020; @ozkaya2022].\n\nMost studies use one-second linear interpolation, but different time frames and styles, such as cubic interpolation, may yield different results.\nCubic spline interpolation produces a smooth curve but may slightly \"overshoot\" measured values [@zhang1997].\nThough the choice of linear vs. cubic interpolation may be small, we recommend authors specify the interpolation type for improved reproducibility.\n\nWhile averaging methods are more frequently reported, about 40% of studies lacked documentation.\nData averaging likely contributes more to the final calculated values of VO~2~max and other variables than do outlier removal and interpolation as averaging suppresses potential outliers itself by combining those points with additional observations.\nResearch on the effect of interpolation on VO~2~ kinetics parameters shows that interpolation does not significantly affect the values of parameter estimates [@benson2017; @francescato2014; @francescato2019; @francescato2015].\nAlthough we are unaware of studies comparing the effect of outlier removal or leaving data as-is before proceeding with other calculations, the known impact of data averaging on VO~2~max and the inherent dampening effect of averaging on outliers itself suggests that data averaging is the most important of the three steps when the goal is to reflect the underlying whole-body metabolic rate.\nTherefore, researchers should state their gas exchange data averaging methods to improve research reproducibility and study comparisons.\n\nStating averaging methods can also help correctly classify cardiorespiratory fitness against normative data.\nResearch by Martin-Rincon et al. [@martin-rincon2019] offers a strategy to compare two VO~2~max values obtained with different averaging methods.\nWithout such corrections, one could misclassify cardiorespiratory fitness based on VO~2~max if VO~2~max were calculated with a sufficiently different sampling interval than that used to generate the normative data.\nHowever, this correction applies to group data [@martin-rincon2019] and different averaging strategies also affect individual VO~2~max values.\n\nImportantly, the normative data offered by the American College of Sports Medicine [@pescatello2014] is based on a regression of VO~2~ vs. time-to-exhaustion using a modified Balke protocol and equations developed from @pollock1982 and @pollock1976 (Cooper Institute, personal communication, 9/2021), rather than directly measured.\nThe system used to create the regression for males [@pollock1976] and females [@pollock1982] averaged the data every minute and every 30 seconds, respectively.\nGiven that, stating the averaging methods used may allow for better comparisons to normative data.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nn_15_breath_roll <- avg_by_full_method_tab %>% \n    filter(avg_type == \"breath\" & avg_subtype == \"rolling\" & avg_amount == 15) %>% \n    select(n) %>% \n    pull()\n\nprop_15_breath_roll <- avg_by_full_method_tab %>% \n    filter(avg_type == \"breath\" & avg_subtype == \"rolling\" & avg_amount == 15) %>% \n    select(prop) %>% \n    pull()\n\nz <- qnorm(0.025, lower.tail = FALSE)\n\n# moe_15_br_rolling <- binom::binom.confint(\n#     x = n_15_breath_roll,\n#     n = n_total_articles_avg,\n#     conf.level = 0.95,\n#     methods = \"ac\")\n# \n#     z * sqrt(\n#     prop_15_breath_roll * (1 - prop_15_breath_roll) / \n#         n_total_articles_avg)\n\nn_low_pass <- avg_by_full_method_tab %>% \n    filter(avg_type == \"digital filter\" & avg_subtype == \"butterworth low-pass\") %>% \n    select(n) %>% \n    pull()\n\nprop_low_pass <- avg_by_full_method_tab %>% \n    filter(avg_type == \"digital filter\" & avg_subtype == \"butterworth low-pass\") %>% \n    select(prop) %>% \n    pull()\n\nmoe_low_pass <- z * sqrt(\n    prop_low_pass * (1 - prop_low_pass) / \n        n_total_articles_avg)\n\ntop_three_full_method <- avg_by_full_method_tab %>% \n    slice_max(prop, n = 3) %>% \n    select(prop) %>% \n    pull()\n\navg_by_full_method_tab_condensed <- avg_by_full_method_tab %>% \n    mutate(avg_procedure = paste(\n        avg_type, avg_subtype, avg_amount, avg_mos, avg_mean_type, sep = \"-\"),\n        avg_procedure = if_else(prop < 0.01, \"Other\", str_to_title(avg_procedure))) %>% \n    group_by(avg_procedure) %>% \n    summarize(n = sum(n)) %>% \n    ungroup() %>% \n    mutate(prop = prop.table(n)) %>% \n    mutate(avg_procedure = str_remove(avg_procedure, \"-Mean-Whole\"),\n           avg_procedure = str_remove_all(avg_procedure, \"-Na\")) %>% \n    arrange(desc(prop))\n\ntop_three_full_method <- avg_by_full_method_tab_condensed %>% \n    slice_max(prop, n = 3) %>% \n    select(avg_procedure, prop) %>% \n    deframe()\n```\n:::\n\n\n\n\n\nThe most frequent, fully specified data averaging method, the 30-second time average, fits the maximum recommended duration by Robergs [-@robergs2010].\nHowever, Robergs et al. recommended a 30-s rolling rather than a bin average.\nThe same study also recommended a 15-breath rolling average or, preferably, the low-pass digital filter, but we only documented these methods two (0.2 and one (0.1%) times, respectively.\nDespite the most popular method not exceeding the maximum recommended duration, at least for calculating VO~2~max, the vast majority of papers neglected to follow current guidelines.\n\nThis review did not document all aspects of the recommending reporting guidelines proposed by Nolte et al. [@nolte2023].\nWe did not record the exact metabolic cart used, the measurement mode, the software used, nor the data processing rationale.\nOur outlier reporting rates of 5 was similar, while our interpolation reporting rate of 7.0% was slightly higher than Nolte's 4.3%.\nOur averaging reporting rate was also \\~5% higher at 60.9.\nOur results are similar to previous works showing that time-bin averages are the most popular, with 30-second averages as the most common overall [@midgley2007; @nolte2023; @robergs2010].\nThe order in popularity of other methods differs from these other studies.\nThis may be attributed to our wider date range and including data processing beyond that to describe VO~2~max.\n\nAlthough more complex data processing strategies have existed for several years, we suspect time-bin averages are still favored in part due to subjective reasons like tradition [@robergs2010].\nWe speculate that practitioners also chose time-bin averages if more complex options are not integrated within metabolic cart software.\n\n## Limitations\n\nWhile extensive, this study's scope limited detailed examination of each article, meaning some data processing descriptions might have been missed due to the limitations of our RegExs, leading us to categorize these as \"not described.\" Articles citing prior works for data processing were marked as \"not described\" for simplicity.\nCiting other work may help meet word counts, but methodological shortcut citations may hinder reproducibility [@standvoss2024].\nThe best practice yet would be for authors to publish all data and code when possible.\nSeveral open-source gas data software are available to facilitate transparent analyses [@whippr; @gasExchangeR; @spiro].\n\nNext, by chance, we found rare examples of articles using the median as the measure of center as we built our RegExs.\nHowever, we did not document any such cases in our random sample.\nA larger random sample would likely find these and other rare data averaging methods.\nAlso, a few ineligible articles may have eluded our screening.\nWe also predict some studies may have used mixing chambers without specifying [@nolte2023].\nTaken together, our results may slightly underestimate data processing methods' true reporting frequency.\n\nAnother limitation of this scoping review is that our results do not indicate how different data processing methods were used.\nFor example, we did not distinguish if a 60-second time-bin average was used to calculate VO~2~max or a steady-state exercise period.\nTherefore, this review cannot estimate the prevalence of different processing methods for specific analyses, such as VO~2~max.\nPrevious research shows that 30-second averages for VO~2~max are the most common [@robergs2010; @midgley2007; @nolte2023], presumably as they remove short-term fluctuations.\nWe anticipate that VO~2~ kinetics and similar analyses which rely on high temporal resolution data likely employ averaging with less smoothing.\nModerate smoothing may assist with detecting ventilatory thresholds as an optimal signal-to-noise ratio may highlight the systematic changepoints between ventilatory variables.\nNevertheless, this is the first study we know of to document data processing methods *besides* those used to calculate VO~2~max.\n\n## Conclusions\n\nThis scoping review found that data processing methods were seldom reported for outlier removal and interpolation, and that averaging reporting, though much higher, could further improve.\nThe results reflect prevalent methods.\nWhile prevalence should not be conflated with quality, knowing prevalent methods allows testing their influence on data processing.\nWe hope these results motivate better methodological documentation and reproducibility in this field.\n\n<!-- # Funding {.unnumbered} -->\n\n<!-- We have no funding sources to disclose. -->\n\n<!-- # Acknowledgments {.unnumbered} -->\n\n<!-- We thank Scott Marsalis and Cody Hennesy from the University -->\n\n<!-- of Minnesota Libraries for their advice and support. -->\n\n<!-- # Conflict of Interest {.unnumbered} -->\n\n<!-- The authors declare that they have no conflict of interest. -->\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n\n**Table and Figure Captions**\n\n**Table Captions**\n\nTable 1a: Averaging type by counts (N) and proportions (%).\n\nTable 1b: Averaging subtype by counts (N) and proportions (%).\n\n**Figure Captions**\n\nFigure 1: Selection of sources of evidence flow diagram per the PRISMA 2020 statement [@page2021].\n\nFigure 2: Counts and percentages of outlier limits when specified.\n\nFigure 3: Most prevalent specified interpolation methods by both type and time.\n\nFigure 4: Prevalence of complete averaging procedures.\nThe numbers in each column label are in seconds for time averages and the number of breaths for breath averages.\nThe \"other\" column represents methods that accounted for less than 1% of the total stated methods.\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}