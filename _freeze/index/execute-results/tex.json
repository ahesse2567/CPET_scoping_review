{
  "hash": "45b7fabd1566fd8d1c0f08cf9dc06dfd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Popularity And Prevalence Of Gas Exchange Data Processing Methods In Peer-reviewed Literature: A Scoping Review\"\nauthor:\n  - name: Anton Hesse\n    orcid: 0000-0001-8456-7343\n    corresponding: true\n    email: hesse151@umn.edu, ahesse2567@gmail.com\n    roles:\n      - Investigation\n      - Software\n      - Visualization\n      - Writing – Original Draft Preparation\n    affiliations:\n      - University of Minnesota-Twin Cities\n  - name: Christopher Lundstrom\n    orcid: 0000-0002-1527-1685\n    corresponding: false\n    roles: \n      - Supervision\n      - Writing – Review & Editing\n    affiliations:\n      - University of Minnesota-Twin Cities\nabstract: |\n  ADD ABSTRACT HERE\nkeywords:\n  - Data averaging\n  - Outlier Removal\n  - Interpolation\n# author-note:\n#   disclosures:\n#     conflict of interest: \"The author has no conflict of interest to declare.\"\ndate: last-modified\nbibliography: references.bib\ncitation:\n  container-title: \"JOURNAL GOES HERE\"\nnumber-sections: true\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.3     v readr     2.1.4\nv forcats   1.0.0     v stringr   1.5.0\nv ggplot2   3.4.3     v tibble    3.2.1\nv lubridate 1.9.3     v tidyr     1.3.0\nv purrr     1.0.2     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(knitr)\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nextrafont::loadfonts(quiet = TRUE)\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nhere() starts at /Users/antonhesse/Desktop/Anton/Education/UMN/PhD/Dissertation/CPET_scoping_review\n```\n\n\n:::\n:::\n\n\n\n\n\n\n# Introduction\n\nClinicians and researchers commonly use cardiopulmonary exercise testing (CPET) to determine maximal aerobic capacity (VO~2~max), the first ventilatory threshold (VT~1~), the respiratory compensation point (RC or RCP), and VO~2~ kinetics parameters.\nSuch values help categorize fitness, predict disease risk, and inform exercise training [@pescatello2014, p. 162].\nUsing these results, especially exercise intensity based on ventilatory thresholds, yields better and more consistent results than using percentages of maximal or reserve anchors, such as %HRmax or %HR reserve, respectively [@jamnick2020].\nThese improved results are thought to occur because exercise relative to these thresholds manifests as similar metabolic and ventilatory responses [@jamnick2020].\nTherefore, incorrectly calculating or identifying these values may limit the exercise prescription benefits derived from a CPET.\n\nCalculating the above values often requires data processing when CPET data is collected breath-by-breath (BBB) as it is highly variable [@robergs2010].\nThe data processing sequence in most studies usually starts with outlier removal, followed by an optional data interpolation step so the data appear at regular time intervals, and ends by averaging the data.\nThis smooths that data to better reflect the underlying whole-body metabolism, which changes more slowly than variable ventilatory data [@robergs2010].Previous research has described common data processing methods, particularly the effect of data averaging, and how these methods influence calculated CPET values.\nFor example averaging the data over longer durations reduces VO~2~max and the frequency of detecting a VO~2~ plateau [@sousa2010; @johnson1998; @sell2021; @midgley2007; @astorino2009; @astorino2000; @sousa2010; @martin-rincon2019; @martin-rincon2020; @scheadler2017; @dejesus2014; @hill2003; @smart2015; @matthews1987; @robergs2003; @robergs2010].\nHowever, there is scarce information on data averaging and its effect on ventilatory threshold identification.\n\nMany studies remove outliers by finding points ±3 or ±4 standard deviations (SD) beyond the local mean (i.e., a prediction interval).\nThese cutoffs are common because the relatively small sample size of BBB gas exchange data often contains more values beyond 3 or 4 SD than one would predict from an assumed Gaussian distribution [@lamarra1987].\nMore outliers appear than expected because of \"swallowing, coughing, or premature ending of the breath for some other reason\" [@lamarra1987].\nWe are unaware of prior research that examines how different outlier removal strategies affect VO~2~max or ventilatory thresholds.\n\nGas exchange data interpolation is primarily used to distribute data points uniformly, often to one-second intervals.\nThis procedure is more common with VO~2~ kinetics analyses because it allows for repeated transitions between easier and harder exercise bouts to be \"ensemble\" averaged by superposition [@keir2014; @lamarra1987].\nWithin VO~2~ kinetics research, the practice of linearly interpolating data every one second has been criticized because it artificially narrows confidence intervals [@benson2017; @francescato2014; @francescato2019; @francescato2015].\nSince ventilation frequency at the beginning of a graded exercise test is approximately 0.2 Hz, interpolating to every one second adds dependent data points that do not represent an independent reading by the gas analyzer and flow sensor.\nThis increases the sample size without adding independent readings and thus artificially narrows the confidence intervals of parameter estimates in VO~2~ kinetics analyses.\nAs before, we are unaware of research specifically investigating how interpolation affects VO~2~max and ventilatory threshold identification.\n\nTherefore, some evidence suggests that data processing choices, such as outlier removal and interpolation, can significantly impact the CPET variables calculated.\nHowever, we are unaware of research investigating the effect of data processing on ventilatory threshold identification.\nSome research has already surveyed reported data averaging in peer-reviewed journals [@midgley2007] and via survey [@robergs2010], with the major finding that time-based bin averages (see @sec-methods for details) were the most popular averaging methods.\nHowever, these two studies were rather small in scale.\nFuture researchers conducting studies on CPET data processing may therefore wish to learn the most popular data processing choices to best represent the field.\n\nBefore conducting this scoping review, we anecdotally observed that many articles using CPET data did not report all data processing steps, especially outlier removal and interpolation details.\nThis may hamper reproduction or replication attempts.\nTherefore, to assist with conducting research on the effects of data processing on CPET values and to evaluate the methodological reproducibility of research using BBB gas exchange data generally, we conducted a broad scoping review to identify the prevalence and popularity of outlier removal, interpolation, and data averaging methods.\n\n# Methods {#sec-methods}\n\n## Design\n\nThis study is a scoping review of CPET data processing methods.\nIt documents the frequency of reporting and the prevalence of reported methods.\nThe choice to perform a scoping review over a systematic review fits with the broader nature of scoping reviews, rather than the narrower questions asked by systematic reviews [@arksey2005].\nThis study specifically seeks to survey the breadth of gas exchange data processing choices in peer-reviewed published literature, rather than ask a specific question about them.\nThe review summarizes the types and frequency of methods used to 1) remove outliers; 2) interpolate data; and 3) average data.\n\n## Protocol Registration\n\nThe methods presented [@peters2020] and reporting of results [@tricco2018] are modeled after the guidelines and checklist described by the Preferred Reporting Items for Systematic reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR).\nThis protocol was registered with the Open Science Framework (OSF) and is available through this link: <https://doi.org/10.17605/OSF.IO/A4VMZ>.\n\n## Eligibility Criteria\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nsource(here(\"code/cpet_articles/analysis/reporting/selection_sources_evidence.R\"))\n```\n:::\n\n\n\n\n\n\nThe eligibility criteria for this review is as follows: (1) peer-reviewed articles; (2) presence of an exercise test with BBB gas exchange data collection; (3) written in English; (4) involved human participants; and (5) open access with a digital object identifier (DOI), or a PubMed unique identifier or PubMed Central reference numbers (PMID or PMCID, respectively) that can be mapped to a DOI.\nSearches were not date limited to maximize the total number of results.\nWe sought peer-reviewed articles as a minimum standard of research quality.\nLimiting to English-only articles reduced our article pool size but simplified our later regular expression analysis.\nFinally, tracing back to a DOI allowed for rapid identification of article metadata and informed our later full-text download strategy.\nWe acknowledge that requiring DOIs induces a bias towards newer articles because older articles may lack them.\nFurthermore, although guidance on scoping reviews suggests including unpublished literature to be as comprehensive as possible [@peters2020], our electronic search returned 21,980 articles.\nWe, therefore, feel that our assessment was sufficiently comprehensive.\n\n## Information Sources and Search\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmanual_downloads <- read_csv(\n    file.path(\"data/cpet_articles/Manual Downloads - Articles.csv\"),\n    show_col_types = FALSE)\n\nn_unsubscribed <- manual_downloads %>% \n    count(subscribed) %>% \n    filter(subscribed == FALSE) %>% \n    select(n) %>% \n    pull()\n```\n:::\n\n\n\n\n\n\nWe acquired data from the Ovid-MEDLINE, Scopus, and Web of Science databases with the guidance of a university librarian and exported search results to Microsoft Excel spreadsheets.\nThe electronic search strategy for the Ovid-MEDLINE database can be found at this link: <https://osf.io/a4vmz/files/osfstorage/6255791f28f9400531a24c96>.\n\nThis output from the electronic search includes different article identifiers, including DOIs, PMIDs, and PMCIDs.\nWe used the PubMed Central ID Converter API [@ncbi2021] to find additional DOIs from PMIDs and PMCIDs where DOIs were missing using Python code.\nWith a more complete list of DOIs, we obtained open-access, full-text articles using the unpywall Python package.\nThis package interfaces with [unpaywall.org](https://unpaywall.org/), a database of open-access articles.\nFor articles unobtainable via unpywall, we also used Python to download full-text articles via publisher text and data mining (TDM) application programming interfaces (API) and to write custom web-scraping software when publishers lacked TDM APIs.\nAny remaining articles were downloaded manually, assuming a valid University subscription.\nOur subscription did not permit us access to 1,549 articles.\n\n## Selection of Sources of Evidence\n\nThis study used a single screening process because it differs from most scoping reviews.\nIt only requires an exercise test with BBB gas exchange data collection rather than a more complex assessment of the overall methodology and intervention.\n\n### Text Analysis and Screening\n\nDespite database search filters, we determined additional screening was necessary after reading a subset of results because we found several non-English, non-human, and non-original research, such as reviews, meta-analyses, and protocol registrations.\nWe also excluded case studies.\nThese ineligible articles were removed from a combination of random forest machine learning classifier models and regular expressions using the Python programming language.\nWe manually analyzed a subset of articles to help build machine learning classifiers and construct regular expressions described below.\n\nFirst, full-text PDF and EPUB documents were converted into plain text (.txt) files.\nBefore text screening, we normalized the text with regular expressions (sequences of characters that specify a search pattern in text) by converting text to lowercase, removing end-of-line hyphenations and excess whitespace, and fixing some errors induced when converting the file type.\n\nAfter text normalization, we removed articles that did not convert correctly to a txt format by identifying empty files.\nThen we identified non-English articles using the fasttext Python module [@bojanowski2016].\nNext, we used a supervised random forest machine learning classifier from the sklearn Python package [@pedregosa2011] to identify ineligible non-human and non-original research.\nWe manually verified ineligible articles identified by the machine learning classifier.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nineligible_articles <- read_csv(\n    \"data/cpet_articles/text_analysis/eligibility/ineligible_articles_combined.csv\",\n    show_col_types = FALSE)\n\nbbb_articles <- read_csv(\n    \"data/cpet_articles/text_analysis/all_bbb_articles.csv\",\n    show_col_types = FALSE) %>% \n    distinct(doi_suffix, .keep_all = TRUE) %>% \n    filter(!(doi_suffix %in% ineligible_articles$doi_suffix))\n\nn_bbb_articles <- nrow(bbb_articles)\n```\n:::\n\n\n\n\n\n\nNext, we identified BBB articles using regular expressions.\nArticles were considered BBB articles if their text contained variations of the phrase \"breath-by-breath\", or if their text included the make or model of a known BBB analyzer.\nBreath-by-breath brands and analyzers we included were Oxycon and Carefusion brands, Medgraphics Ultima, CPX, CCM, and CardiO~2~ models, Sensormedics Encore and 2900 models, Cosmed quark, k4, and k5 models, and the Minato RM-200, AE-280S, AE-300S, and AE-310S models.\nIn total, we identified 8,417 articles.\n\nWithin this subset, we performed a similar regular expression search for studies that documented using Douglas Bags or mixing chambers and excluded those articles.\nThe full details are described in the \"data charting process\" section.\n\n### Data Charting Process\n\nWe designed our regular expressions by manually reading many full-text articles and documenting common written patterns describing outlier, interpolation, and averaging methods.\nWe first identified the presence of short phrases that usually indicated that the study authors included details on outlier removal, interpolation, or data averaging steps in their methods.\nIf present, we extracted a snippet of text surrounding those phrases for later manual analysis by obtaining approximately 200 characters before and after the short phrase.\nThe methods from these snippets were then recorded.\nIn all cases, methods were only considered documented if the snippets provided at least some specific information.\nFor example, articles stating outlying breaths were removed but without describing the criteria for determining outliers were considered \"not described.\" Examples of common phrases and snippets are provided for each data processing category in the outlier, interpolation, and averaging method subsections below.\nFinally, when a snippet was ambiguous, we read the full-text file to accurately document the data processing methods.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nz <- qnorm(0.025, lower.tail = FALSE)\nmargin_of_error <- 0.03\np <- 0.5\n\nn_articles <- ceiling(((margin_of_error / z)^2)^-1 * (p * (1 - p)))\n\nn_rounded_up_nearest_hundred <- round(n_articles, -2)\n```\n:::\n\n\n\n\n\n\nWe analyzed all eligible BBB articles for outlier and interpolation methods because fewer articles described these methods (\\~5%) and the phrases were more distinct.\nIn contrast, we analyzed a random subset of articles to document data averaging methods because far more articles described their averaging methods.\nEarly estimates as we developed our regular expressions were that \\~60% or 5,050 articles had some averaging details.\nFurthermore, the phrases associated with averaging methods are more generic and often refer to other study aspects, such as heart rate averaging periods.\nGiven the large number of articles, we needed a minimum sample size of 1,068 based on a 95% confidence interval and a maximum margin of error of ±3%, assuming a proportion of 0.5.\nHowever, we raised this to 1,100 in anticipation of finding ineligible articles that eluded our previous text screening.\n\n#### Outliers\n\nWe supply text-extraction examples in this and the following subsections on interpolation and averaging methods.\nExtracted texts are shown exactly as they were identified after prior text normalization.\nThat is, all text was first converted to lowercase and we removed any end-of-line hyphenations as well as excessive white space repetition.\nWe later capitalized some keywords or phrases to aid our manual reading.\nTherefore, some example snippets do not follow conventional formatting.\nFurthermore, some example snippets may contain non-standard spacing and unicode characters, and may not begin or end at the start or termination of a sentence.\n\nOur outlier regular expressions identified phrases like \"swallowing\", \"coughing\", \"errant\", \"aberrant\", and rerences to the \"local mean,\" \"prediction interval,\" or a specific standard deviation limit such as ±3 or ±4.\nFor example, our regular expressions detected the following phrases from @breese2019: \" errant\"; \" local mean\"; and \"breath-by-breath ̇vo2 data from each step transition were initially edited to exclude errant breaths by removing values lying more than 4 sd.\" After gathering the snippets surrounding those phrases and combining them when they overlapped, we extracted\n\n> > y\\[hb+mb\\] data (quaresima & ferrari, 2009).\n> > expressed as 2.5 data analysis and kinetic modelling the breath-by-breath ̇vo2 data from each step transition were initially edited to exclude errant breaths by removing values lying more than 4 sd from the local mean determined using a five-breath rolling\\\\n\\\\x0c1932 breese et al. and deoxy\\[hb+mb\\] responses were subavera\n\nWe recorded the outlier limit as ±4 SD and the outlier function as a rolling 5-breath whole mean average.\n\n#### Interpolation\n\nNearly all articles describing interpolation methods used variations of \"interpolate.\" The remaining phrases were infrequent and inconsistent enough that interpolation methods were only described for those articles when discovered by chance.\nTo illustrate interpolation documentation, our regular expressions extracted the snippet from @hartman2018.\n\n> > the v̇ o2 data from gd and gl exercise bouts were modeled to characterize the oxygen uptake kinetics following the methods described by bell et al. (2001).\n> > breath-by-breath v̇ o2 data were linearly INTERPOLATed to provide second-by-second values.\n> > phase 1 data (i.e. the cardiodynamic component), from the first ∼20 s of exercise, were omitted from the kinetics analysis because phase 1 is not directly repres\n\nWe documented the interpolation type as \"linear\" and the interpolation time as one second.\n\n#### Averaging\n\n::: {.content-hidden when-format=\"docx\"}\n\n\n\n\n\n::: {#cell-fig-avg_methods_flowchart .cell}\n\n```{.r .cell-code .hidden}\navg_methods_flowchart <- DiagrammeR::grViz(\"\n    digraph avg_methods_fch {\n        # graph statements\n        graph [rankdir = LR, newrank=true];\n        \n        # node defaults\n        node [shape = oval];\n    \n        subgraph cluster_type_unit {\n            # node [rank = same];\n            'Time';\n            'Breath';\n            'Digital\\nFilter';\n        }\n        \n        subgraph cluster_subtype_calc{\n            # node [rank = same];\n            'Bin';\n            'Rolling';\n            'Filter\\nType';\n        }\n        \n        subgraph cluster_measure{\n            # node [rank = same];\n            'Mean';\n            'Median';\n            'Filter\\nSpecs';\n        }\n        \n        subgraph cluster_mean_type{\n            # node [rank = same];\n            'Whole';\n            'Trimmed';\n        }\n        \n        subgraph cluster_steps {\n        node [shape = plaintext]\n            'Type/Unit';\n            'Subtype/Calculation';\n            'Measure\\nof Center';\n            'Mean Type';\n        }\n        \n        'Combination\\nMethod' [shape = box];\n        \n        # edge statments\n        \n        'Type/Unit' -> 'Subtype/Calculation' -> 'Measure\\nof Center' -> 'Mean Type'\n        \n        'Digital\\nFilter':e -> 'Filter\\nType':w [color = blue]\n        {'Time' 'Breath'} -> {'Bin' 'Rolling'} [constraint=true; color=blue];\n\n        'Filter\\nType' -> 'Filter\\nSpecs' [color=red]\n        {'Bin' 'Rolling'} -> {'Mean' 'Median'} [constraint=true; color=red];\n        \n        'Mean':e -> 'Whole':w [constraint=true; color=green]\n        'Mean':e -> 'Trimmed':w [constraint=true; color=green]\n        \n        {rank=same; 'Whole'; 'Trimmed'; 'Combination\\nMethod'}\n        \n        'Whole':e -> 'Combination\\nMethod' [constraint=true]\n        'Trimmed' -> 'Combination\\nMethod' [constraint=true]\n        'Mean':e -> 'Combination\\nMethod' [constraint=true]\n        'Median' -> 'Combination\\nMethod' [constraint=true]\n        \n        'Combination\\nMethod':s -> 'Time':s [constraint=false]\n        'Combination\\nMethod':s -> 'Breath':s [constraint=false]\n   \n    }\n\")\n\navg_methods_flowchart\n```\n\n::: {.cell-output-display}\n![Flowchart depicting the four major components of averaging method documentation. Colored arrows are used to visually distinguish each step.](index_files/figure-pdf/fig-avg_methods_flowchart-1.pdf){#fig-avg_methods_flowchart fig-pos='H'}\n:::\n:::\n\n\n\n\n\n:::\n\nWe chose to document averaging methods according to the following five criteria: type/units, subtype/calculation, amount, measure of center, and mean type ([@fig-avg_methods_flowchart]).\nType/units refers to the averaging units, including time, breath, and digital filter.\nSubtype/calculation corresponds to the computation, such as bin averages, rolling averages, or the specific form of the digital filter.\nThe amount is the unit quantity.\nFor example, the amount of 30 for a time average is 30 seconds, but it is 30 breaths for a breath average.\nThe measure of center refers to the mean or median.\nFinally, the mean type specifies a whole vs. a trimmed mean.\nTrimmed (truncated) refers to removing a percentage or number of the highest and lowest values before calculating the average of the remaining data to minimize the effect of outliers.\n\nDescriptions of averaging methods are also considerably more diverse and generic than outlier and interpolation descriptions.\nFor example, \"30-second averages\" and \"averaged every 30 seconds\" invite complexity, leading to more snippets referring to averaging something besides BBB gas exchange data.\nGiven that, we required that the text snippets include a reference to gas data such as the text \"O~2~,\" \"breath,\" \"gas,\" \"ventilation,\" etc.\n\nIn contrast to previous studies, we also documented every averaging method we found per paper instead of only describing the averaging method for VO~2~max.\nWe also recorded multiple averaging methods when the authors described the sampling interval and the transformation applied to it.\nFor example, the snippet from @hassinen2008\n\n> > ath method using the vmax respiratory gas analyzer (sensormedics, yorba linda, ca).\n> > vo2max was deﬁned as the mean of the three highest values of the averaged oxygen consumption measured consecutively OVER 20-S intervals.\n> > a total of 98% of the subjects achieved the respiratory exchange ratio of ⱖ1.1.\n> > electrocardiography was recorded throughout the exercise test using cardiosoft software (ge medical systems,\n\nstates that oxygen consumption was measured every 20 seconds and that VO~2~max was calculated as the average of three 20-second intervals, or 60-seconds.\nFor this article, we documented one averaging method as a 20-second time bin whole mean and another as a 60-second time bin whole mean.\n\nIn many cases, authors did not explicitly use the terms \"average\" or \"mean\" to describe their averaging methods, but we documented their methods when implied.\nFor example, the snippet from @deboeck2004 reading\n\n> > red using a continuously monitored electrocardiograph.\n> > blood pressure was measured at the end of each workload increment using an automatic sphygmomanometer.\n> > peak v9o2 was deﬁned as the v9o2 measured DURING THE LAST 30 S of peak exercise.\n> > oxygen pulse was calculated by dividing v9o2 by cardiac frequency.\n> > the anaerobic threshold was detected using the v-slope method \\[16\\].\n> > the ventilatory equivalent for carbon dioxide w\n\nstates they calculated VO~2~peak using the last 30 seconds of exercise data.\nWe documented such phrasing as a 30-second time-bin whole mean average.\n\n### Data Items\n\nIn all cases, articles that did not return any phrases were documented as \"not described\" for their respective data processing category.\nIf snippets did not refer to the data processing category or if the snippet lacked sufficient information, those data processing variables were documented as \"not described.\" For example, interpolation variables were denoted as \"not described\" if interpolation was acknowledged but without details for the interpolation type or time.\n\n#### Outliers\n\nWe documented the outlier limit, for example, ±3 standard deviations, and any outlier function used to compute the outlier limit, if described.\n\n#### Interpolation\n\nWe recorded the interpolation type (linear, cubic, Lagrange, specifically *un*interpolated, and other) and time frame (e.g., every one second).\n\n#### Averaging\n\nWe noted the following averaging types: Time, breath, breath-time, time-breath, digital filter, ensemble, (explicitly) *un*averaged, and other.\nAveraging subtypes included bin, rolling, bin-roll, rolling-bin, Butterworth low-pass, Fast Fourier Transform, and Savitsky-Golay.\nNext, we recorded the time in seconds or the number of breaths.\nWe recorded the measure of center as mean or median.\nFinally, we noted if the mean was a whole or trimmed.\n\n### Synthesis of Results\n\nWe reported the counts, percentages, and margin of error (95% confidence) of the reporting frequency for each data processing method.\nWe calculated the counts and percentages of reported methods for those studies reporting their data processing.\nThese descriptive statistics were computed using R [@rcoreteam2021] and RStudio [@positteam2022].\n\n# Results {#sec-results}\n\n# Discussion {#sec-discussion}\n\n# References {.unnumbered}\n\n::: {#refs}\n:::\n",
    "supporting": [
      "index_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "\\usepackage{booktabs}\n\\usepackage{longtable}\n\\usepackage{array}\n\\usepackage{multirow}\n\\usepackage{wrapfig}\n\\usepackage{float}\n\\usepackage{colortbl}\n\\usepackage{pdflscape}\n\\usepackage{tabu}\n\\usepackage{threeparttable}\n\\usepackage{threeparttablex}\n\\usepackage[normalem]{ulem}\n\\usepackage{makecell}\n\\usepackage{xcolor}\n"
      ]
    },
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}